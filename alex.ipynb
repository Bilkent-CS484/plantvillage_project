{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-16T20:30:01.264712100Z",
     "start_time": "2024-04-16T20:30:00.858075500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54305\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    " #   transforms.Resize(256),             # they are already resized in processing\n",
    "    transforms.CenterCrop(227),       # AlexNet input size is 227\n",
    "    transforms.ToTensor(),             \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalization\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder('color', transform=transform)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "validation_size = len(dataset) - train_size\n",
    "train_dataset, validation_dataset = random_split(dataset, [train_size, validation_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from torchvision.models import AlexNet_Weights\n",
    "from torchvision import models\n",
    "#pretrained alexnet model to test\n",
    "model = models.alexnet(weights=AlexNet_Weights.IMAGENET1K_V1)\n",
    "\n",
    "model.classifier[6] = torch.nn.Linear(model.classifier[6].in_features, 38)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T20:30:02.090240200Z",
     "start_time": "2024-04-16T20:30:01.255714700Z"
    }
   },
   "id": "ca7bea57a3a30377"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10, Loss: 3.588106155395508\n",
      "Batch 20, Loss: 3.5481505393981934\n",
      "Batch 30, Loss: 3.5763235092163086\n",
      "Batch 40, Loss: 3.259951114654541\n",
      "Batch 50, Loss: 3.231973648071289\n",
      "Batch 60, Loss: 3.427610158920288\n",
      "Batch 70, Loss: 3.315734386444092\n",
      "Batch 80, Loss: 3.3667454719543457\n",
      "Batch 90, Loss: 3.4248456954956055\n",
      "Batch 100, Loss: 3.3846139907836914\n",
      "Batch 110, Loss: 3.2260706424713135\n",
      "Batch 120, Loss: 3.2688100337982178\n",
      "Batch 130, Loss: 3.332181453704834\n",
      "Batch 140, Loss: 3.2506771087646484\n",
      "Batch 150, Loss: 3.6070151329040527\n",
      "Batch 160, Loss: 3.3267838954925537\n",
      "Batch 170, Loss: 3.171787977218628\n",
      "Batch 180, Loss: 3.3601369857788086\n",
      "Batch 190, Loss: 3.4935195446014404\n",
      "Batch 200, Loss: 3.331143379211426\n",
      "Batch 210, Loss: 3.1557068824768066\n",
      "Batch 220, Loss: 3.1612601280212402\n",
      "Batch 230, Loss: 3.4984817504882812\n",
      "Batch 240, Loss: 3.3776192665100098\n",
      "Batch 250, Loss: 3.494133234024048\n",
      "Batch 260, Loss: 3.17972993850708\n",
      "Batch 270, Loss: 3.3125667572021484\n",
      "Batch 280, Loss: 3.493208885192871\n",
      "Batch 290, Loss: 3.4649643898010254\n",
      "Batch 300, Loss: 3.44423770904541\n",
      "Batch 310, Loss: 3.4288198947906494\n",
      "Batch 320, Loss: 3.4452157020568848\n",
      "Batch 330, Loss: 3.458280324935913\n",
      "Batch 340, Loss: 3.3957180976867676\n",
      "Batch 350, Loss: 3.1439549922943115\n",
      "Batch 360, Loss: 3.3982486724853516\n",
      "Batch 370, Loss: 3.6050949096679688\n",
      "Batch 380, Loss: 3.469794273376465\n",
      "Batch 390, Loss: 3.249380588531494\n",
      "Batch 400, Loss: 3.4169528484344482\n",
      "Batch 410, Loss: 3.080260753631592\n",
      "Batch 420, Loss: 3.376814365386963\n",
      "Batch 430, Loss: 3.3366129398345947\n",
      "Batch 440, Loss: 3.5021073818206787\n",
      "Batch 450, Loss: 3.382925510406494\n",
      "Batch 460, Loss: 3.0695478916168213\n",
      "Batch 470, Loss: 3.335357904434204\n",
      "Batch 480, Loss: 3.5142807960510254\n",
      "Batch 490, Loss: 3.4410905838012695\n",
      "Batch 500, Loss: 3.2652411460876465\n",
      "Batch 510, Loss: 3.3468120098114014\n",
      "Batch 520, Loss: 3.2697947025299072\n",
      "Batch 530, Loss: 3.4424681663513184\n",
      "Batch 540, Loss: 3.619448184967041\n",
      "Batch 550, Loss: 3.4192118644714355\n",
      "Batch 560, Loss: 3.2763209342956543\n",
      "Batch 570, Loss: 3.3197383880615234\n",
      "Batch 580, Loss: 3.5097458362579346\n",
      "Batch 590, Loss: 3.441535472869873\n",
      "Batch 600, Loss: 3.4451372623443604\n",
      "Batch 610, Loss: 3.223966360092163\n",
      "Batch 620, Loss: 3.3767471313476562\n",
      "Batch 630, Loss: 3.3351809978485107\n",
      "Batch 640, Loss: 3.4965991973876953\n",
      "Batch 650, Loss: 3.223886013031006\n",
      "Batch 660, Loss: 3.1771392822265625\n",
      "Batch 670, Loss: 3.3669517040252686\n",
      "Batch 680, Loss: 3.4128527641296387\n",
      "Batch 690, Loss: 3.5536584854125977\n",
      "Batch 700, Loss: 3.4137823581695557\n",
      "Batch 710, Loss: 3.3837172985076904\n",
      "Batch 720, Loss: 3.737748146057129\n",
      "Batch 730, Loss: 3.281609058380127\n",
      "Batch 740, Loss: 3.328256130218506\n",
      "Batch 750, Loss: 3.4099035263061523\n",
      "Batch 760, Loss: 3.506042957305908\n",
      "Batch 770, Loss: 3.255519390106201\n",
      "Batch 780, Loss: 3.43058180809021\n",
      "Batch 790, Loss: 3.2218265533447266\n",
      "Batch 800, Loss: 3.2101528644561768\n",
      "Batch 810, Loss: 3.473508358001709\n",
      "Batch 820, Loss: 3.288595199584961\n",
      "Batch 830, Loss: 3.3168790340423584\n",
      "Batch 840, Loss: 3.2041895389556885\n",
      "Batch 850, Loss: 3.4624836444854736\n",
      "Batch 860, Loss: 3.5483715534210205\n",
      "Batch 870, Loss: 3.492830514907837\n",
      "Batch 880, Loss: 3.4321236610412598\n",
      "Batch 890, Loss: 3.3781282901763916\n",
      "Batch 900, Loss: 3.3979010581970215\n",
      "Batch 910, Loss: 3.210815906524658\n",
      "Batch 920, Loss: 3.4636034965515137\n",
      "Batch 930, Loss: 3.108372926712036\n",
      "Batch 940, Loss: 3.270699977874756\n",
      "Batch 950, Loss: 3.039783239364624\n",
      "Batch 960, Loss: 3.16721773147583\n",
      "Batch 970, Loss: 3.397838830947876\n",
      "Batch 980, Loss: 3.3696043491363525\n",
      "Batch 990, Loss: 3.3817989826202393\n",
      "Batch 1000, Loss: 3.380126714706421\n",
      "Batch 1010, Loss: 3.179180383682251\n",
      "Batch 1020, Loss: 3.3500194549560547\n",
      "Batch 1030, Loss: 3.2771475315093994\n",
      "Batch 1040, Loss: 3.1452555656433105\n",
      "Batch 1050, Loss: 3.334279775619507\n",
      "Batch 1060, Loss: 3.4486141204833984\n",
      "Batch 1070, Loss: 3.48781418800354\n",
      "Batch 1080, Loss: 3.366050958633423\n",
      "Batch 1090, Loss: 3.4501290321350098\n",
      "Batch 1100, Loss: 3.310116767883301\n",
      "Batch 1110, Loss: 3.182480812072754\n",
      "Batch 1120, Loss: 3.492204189300537\n",
      "Batch 1130, Loss: 3.317131996154785\n",
      "Batch 1140, Loss: 3.3556206226348877\n",
      "Batch 1150, Loss: 3.6425464153289795\n",
      "Batch 1160, Loss: 3.22703218460083\n",
      "Batch 1170, Loss: 3.3513169288635254\n",
      "Batch 1180, Loss: 3.170368194580078\n",
      "Batch 1190, Loss: 3.380732297897339\n",
      "Batch 1200, Loss: 3.5099287033081055\n",
      "Batch 1210, Loss: 3.532371759414673\n",
      "Batch 1220, Loss: 3.535348653793335\n",
      "Batch 1230, Loss: 3.1147258281707764\n",
      "Batch 1240, Loss: 3.5585992336273193\n",
      "Batch 1250, Loss: 3.635241985321045\n",
      "Batch 1260, Loss: 3.443141460418701\n",
      "Batch 1270, Loss: 3.386229991912842\n",
      "Batch 1280, Loss: 3.1758344173431396\n",
      "Batch 1290, Loss: 3.34850811958313\n",
      "Batch 1300, Loss: 3.314833879470825\n",
      "Batch 1310, Loss: 3.160536289215088\n",
      "Batch 1320, Loss: 3.6291825771331787\n",
      "Batch 1330, Loss: 3.323185443878174\n",
      "Batch 1340, Loss: 3.039785385131836\n",
      "Batch 1350, Loss: 3.3591439723968506\n",
      "Training complete and model saved\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "model.train()\n",
    "\n",
    "for i, (inputs, labels) in enumerate(train_loader):\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f'Batch {i + 1}, Loss: {loss.item()}')\n",
    "\n",
    "torch.save(model.state_dict(), 'alexnet_plantvillage.pth')\n",
    "print('Training complete and model saved')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T20:32:54.311386300Z",
     "start_time": "2024-04-16T20:30:02.093236200Z"
    }
   },
   "id": "4f5882ac0a473d65"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 10.30291869993555%\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model():\n",
    "    model.eval()  # Ensure the model is in evaluation mode.\n",
    "    model.cuda()  # Move model to GPU.\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # Disable gradient computation for inference.\n",
    "        for inputs, labels in validation_loader:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()  # Move inputs and labels to GPU.\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy: {accuracy}%')\n",
    "\n",
    "evaluate_model()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T21:07:14.470421800Z",
     "start_time": "2024-04-16T21:06:21.339834600Z"
    }
   },
   "id": "7638f5d6b972aa94"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
